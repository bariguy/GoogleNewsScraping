{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gonna request Google News, searching for Corona in Korean\n",
    "###### Just for fun :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### from google news :\n",
    "\n",
    "https://news.google.com/search?q=%EC%BD%94%EB%A1%9C%EB%82%98&hl=ko&gl=KR&ceid=KR%3Ako\n",
    "\n",
    " -> \"https://news.google.com\" + \"/search?q=%EC%BD%94%EB%A1%9C%EB%82%98&hl=ko&gl=KR&ceid=KR%3Ako\" (Searching for: 코로나(Corona))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://news.google.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_url = \"/search?q=%EC%BD%94%EB%A1%9C%EB%82%98&hl=ko&gl=KR&ceid=KR%3Ako\"\n",
    "url = base_url + search_url\n",
    "url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(url)\n",
    "response                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "news_items = soup.find_all(\"div\", {\"class\":\"xrnccd\"})\n",
    "news_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Focusing on the second slot from Google News:\n",
    "## link\n",
    "news_items[1].find('a', attrs={'class':'VDXfz'}).get('href')\n",
    "\n",
    "## title\n",
    "news_items[1].find('a', attrs={'class':'DY5T1d'}).text\n",
    "\n",
    "## content\n",
    "news_items[1].find('span', attrs={'class':'xBbh9'}).text\n",
    "\n",
    "## network that reported the news \n",
    "news_items[1].find('a', attrs={'class':'wEwyrc AVN2gc uQIVzc Sksgp'}).text\n",
    "\n",
    "## datetime\n",
    "news_items[1].find('time', attrs={'class':'WW6dff uQIVzc Sksgp'})\n",
    "news_items[1].find('time', attrs={'class':'WW6dff uQIVzc Sksgp'}).get('datetime')\n",
    "news_items[1].find('time', attrs={'class':'WW6dff uQIVzc Sksgp'}).get('datetime').split('T')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export above attributes\n",
    "for item in news_items[:2]:\n",
    "    ## link:\n",
    "    link = item.find('a', attrs={'class':'VDXfz'}).get('href')\n",
    "    news_link = base_url + link\n",
    "    print(news_link)\n",
    "    \n",
    "    ## title:\n",
    "    news_title = item.find('a', attrs={'class':'DY5T1d'}).getText()  # text\n",
    "    print(news_title)\n",
    "    \n",
    "    ## content:\n",
    "    news_content = item.find('span', attrs={'class':'xBbh9'}).getText()  # text\n",
    "    print(news_content)\n",
    "    \n",
    "    ## network:\n",
    "    news_agency = item.find('a', attrs={'class':'wEwyrc AVN2gc uQIVzc Sksgp'}).getText()  # text\n",
    "    print(news_agency)\n",
    "    \n",
    "    ## datetime:\n",
    "    news_date = item.find('time', attrs={'class':'WW6dff uQIVzc Sksgp'}).get('datetime').split('T')[0]\n",
    "    news_time = item.find('time', attrs={'class':'WW6dff uQIVzc Sksgp'}).get('datetime').split('T')[-1]\n",
    "    print(news_date, news_time)\n",
    "    \n",
    "    print('**--------------------------------------------------------**')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's create a function of above steps then.\n",
    "\n",
    "def google_news_clip(url, count=3):\n",
    "    \n",
    "    base_url = \"https://news.google.com\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    news_items = soup.select('div.xrnccd')\n",
    "    \n",
    "    links = []; title = []; content = []; agency = []; info_date = []; info_time = [];\n",
    "    \n",
    "    for item in news_items[:count]:\n",
    "        link = item.find('a', attrs={'class':'VDXfz'}).get('href')\n",
    "        news_link = base_url + link\n",
    "        links.append(news_link)\n",
    "\n",
    "        news_title = item.find('a', attrs={'class':'DY5T1d'}).getText()  # text\n",
    "        title.append(news_title)\n",
    "\n",
    "        news_content = item.find('span', attrs={'class':'xBbh9'}).getText()  # text\n",
    "        content.append(news_content)\n",
    "\n",
    "        news_agency = item.find('a', attrs={'class':'wEwyrc AVN2gc uQIVzc Sksgp'}).getText()  # text\n",
    "        agency.append(news_agency)\n",
    "\n",
    "        news_date = item.find('time', attrs={'class':'WW6dff uQIVzc Sksgp'}).get('datetime').split('T')[0]\n",
    "        news_time = item.find('time', attrs={'class':'WW6dff uQIVzc Sksgp'}).get('datetime').split('T')[-1]\n",
    "        info_date.append(news_date)\n",
    "        info_time.append(news_time)\n",
    "        \n",
    "    result = {'link':links, 'title':title, 'content':content, 'agency':agency, 'date':info_date, 'time':info_time}\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "news = google_news_clip(url, 5)\n",
    "print(news)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets make the keyword as input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 검색어 요청 :\n",
    "input_word = input('Search for: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Google news 검색어 URL 코드로 encoding:\n",
    "urllib.parse.quote(input_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 검색어 정보 URL 구성:\n",
    "base_url = \"https://news.google.com\"\n",
    "keyword = urllib.parse.quote(input_word)\n",
    "search_url = \"/search?q=\" + keyword + \"&hl=ko&gl=KR&ceid=KR%3Ako\"  # keyword: %EC%BD%94%EB%A1%9C%EB%82%98\n",
    "url = base_url + search_url\n",
    "print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's create a function again\n",
    "\n",
    "\n",
    "def google_news_clip_keyword(input_word, count=3):\n",
    "    \n",
    "    base_url = \"https://news.google.com\"\n",
    "    keyword = urllib.parse.quote(input_word)\n",
    "    search_url = \"/search?q=\" + keyword + \"&hl=ko&gl=KR&ceid=KR%3Ako\"\n",
    "    url = base_url + search_url\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    news_items = soup.select('div.xrnccd')\n",
    "    \n",
    "    links = []; title = []; content = []; agency = []; info_date = []; info_time = [];\n",
    "    \n",
    "    for item in news_items[:count]:\n",
    "\n",
    "        link = item.find('a', attrs={'class':'VDXfz'}).get('href')\n",
    "        news_link = base_url + link\n",
    "        links.append(news_link)\n",
    "\n",
    "        news_title = item.find('a', attrs={'class':'DY5T1d'}).getText()  # text\n",
    "        title.append(news_title)\n",
    "\n",
    "        news_content = item.find('span', attrs={'class':'xBbh9'}).getText()  # text\n",
    "        content.append(news_content)\n",
    "        \n",
    "        news_agency = item.find('a', attrs={'class':'wEwyrc AVN2gc uQIVzc Sksgp'}).getText()  # text\n",
    "        agency.append(news_agency)\n",
    "\n",
    "        news_date = item.find('time', attrs={'class':'WW6dff uQIVzc Sksgp'}).get('datetime').split('T')[0]\n",
    "        news_time = item.find('time', attrs={'class':'WW6dff uQIVzc Sksgp'}).get('datetime').split('T')[-1]\n",
    "        info_date.append(news_date)\n",
    "        info_time.append(news_time)\n",
    "        \n",
    "    result = {'link':links, 'title':title, 'content':content, 'agency':agency, 'date':info_date, 'time':info_time}\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Test it \n",
    "\n",
    "\n",
    "input_word = input('검색어 입력 : ')\n",
    "\n",
    "news = google_news_clip_keyword(input_word, 5)\n",
    "print(news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news['link']\n",
    "news['agency']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
